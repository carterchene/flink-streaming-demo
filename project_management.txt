
Need to create dagster pipelines that read from s3 parquets to tables in redshift.
Perhaps use a dagster sensor that looks for when the next directory partition is created, when it is, load the last partition. 
Or can do easy way and just load the last hour every hour 

Once loaded into redshift, create dbt models that build out facts and dims. 

once those are good, lets set up metabase and create some viz. 

Finally, we want to terraformize the whole thing so we can run a single terraform script to build all cloud resources and tear them down as well 

Then, lets create a bunch fo markdown files describing the whole thing. Create a more involved architecture diagram as well.








